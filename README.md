# Distributed-Processing-PySpark

Description

A task is the following:

Learn basics of Distributed computing and PySpark (RDD, map-reduce concept, SparkContext, etc.)

Create a full ML pipeline using PySpark on a selected dataset

Create similar pipeline but without distributed computing

Compare time benchmarks for different dataset size and share findings with the team.

(Optional) Do benchmarks comparison on other datasets.


Definition of Done:
1. A project with ML pipeline for selected dataset is created and placed in personal github.
2. The project uses distributed computing (PySpark).
3. Comparison time benchmarks (for distributed and non-distributed pipelines) for different data sizes is added (in form of a graph where X-axis is data size, Y-axis is time)
4. Presentation (that covers most important Distributed computing & PySpark concepts as well as benchmarks and takeaways if any) is created and shared with colleagues.

Deadline: Friday, April 21
